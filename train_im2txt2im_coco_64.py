#! /usr/bin/python
# -*- coding: utf8 -*-




"""Generate captions for images by a given model."""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import math
import os
import numpy as np
import scipy
import time
from PIL import Image
import logging
LOG_FILENAME = 'record.log'
logging.basicConfig(filename=LOG_FILENAME, level=logging.DEBUG)

import tensorflow as tf
import tensorlayer as tl
from tensorlayer.prepro import *
from tensorlayer.layers import *
import nltk
from model_im2txt import * #<--- Modify from Image-Captioning repo
from utils import *

## Config your image directory here
images_train_dir = '/home/haodong/Workspace/image_captioning/data/mscoco/raw-data/train2014/'
## Config the vocabulary and model of you image captioning module
DIR = "/home/haodong/Workspace/image_captioning"
# Directory containing model checkpoints.
CHECKPOINT_DIR = DIR + "/model/train"
# Vocabulary file generated by the preprocessing script.
VOCAB_FILE = DIR + "/data/mscoco/word_counts.txt"

# load image files list
images_train_list = tl.files.load_file_list(path=images_train_dir, regx='\\.jpg', printable=False)
images_train_list = [images_train_dir + s for s in images_train_list]
images_train_list = np.asarray(images_train_list)
n_images_train = len(images_train_list)

save_dir = "checkpoint" # <-- store the GAN model
if not os.path.exists(save_dir):
    print("[!] Folder %s is not exist, create it." % save_dir)
    os.mkdir(save_dir)
sample_dir = "samples"  # <-- store generated images from seed
if not os.path.exists(sample_dir):
    print("[!] Folder %s is not exist, create it." % sample_dir)
    os.mkdir(sample_dir)

tf.logging.set_verbosity(tf.logging.INFO) # Enable tf.logging

## config : https://github.com/reedscot/icml2016/blob/master/scripts/train_coco.sh
image_size = 64     # image size for txt2im
vocab_size = 12000          # vocabulary size
word_embedding_size = 512   # word embedding size,
rnn_hidden_size = 256       # lstm hidden size
K = 300             # embedding size for image and text mapping
keep_prob = 0.7     # PTB uses 2 LSTM 512, dp 0.5
z_dim = 100        # noise dimension
t_dim = 128        # text feature dimension # paper said 128 : https://github.com/reedscot/icml2016/blob/master/main_cls.lua  NT : https://github.com/reedscot/icml2016/blob/master/scripts/train_coco.sh
c_dim = 3           # for rgb

def prepro_img(x, mode='for_image_caption'):
    if mode=='random_size_to_346':
        x = imresize(x, size=[resize_height, resize_width], interp='bilinear', mode=None)   # <-- random size to (346, 346)
    elif mode=='346_central_crop_to_299':
        ## see model.py: image captioning preprocess the images as below.
        # image = tf.image.resize_images(image,
        #                                size=[resize_height, resize_width],
        #                                method=tf.image.ResizeMethod.BILINEAR)
        # image = tf.image.resize_image_with_crop_or_pad(image, [image_height, image_width, 3]) <-- central crop
        # image = tf.sub(image, 0.5)
        # image = tf.mul(image, 2.0)
        # x = imresize(x, size=[resize_height, resize_width], interp='bilinear', mode=None)   # <-- random size to (346, 346)
        x = crop(x, wrg=image_width, hrg=image_height, is_random=False)                     # <-- (346, 346) to (299, 299) central crop
        x = x / (255. / 2.)
        x = x - 1.
    elif mode=='read_image':
        # x : file name
        x = scipy.misc.imread(x, mode='RGB')
    elif mode=='346_random_flip_crop_to_299':
        # for txt2im
        # x = imresize(x, size=[resize_height, resize_width], interp='bilinear', mode=None)   # <-- random size to (346, 346)
        x = flip_axis(x, axis=1, is_random=True)
        # x = rotation(x, rg=5, is_random=True, fill_mode='nearest')    # <-- no rotation for im2txt
        x = crop(x, wrg=image_width, hrg=image_width, is_random=True) # <-- (346, 346) to (299, 299)
        # x = imresize(x, size=[image_size, image_size], interp='bilinear', mode=None)    # <-- (299, 299) to (64, 64)
        x = x / (255. / 2.)
        x = x - 1.
    elif mode=='resize_to_64':
        # x = rotation(x, rg=5, is_random=True, fill_mode='nearest')   # https://github.com/reedscot/icml2016/blob/master/data/donkey_folder_coco.lua
        x = imresize(x, size=[image_size, image_size], interp='bilinear', mode=None)    # <-- (346, 346) to (64, 64)
        x = x / (255. / 2.)
        x = x - 1.
    else:
        raise Exception("Unsupport mode %s" % mode)
    return x

def sample_fn(softmax_output, top_k, vocab):
    a_id = tl.nlp.sample_top(softmax_output, top_k=top_k)
    word = vocab.id_to_word(a_id)
    return a_id, word

## image-text mapping network
def cnn(input_imgs, is_train, reuse):
    from tensorflow.contrib.slim.python.slim.nets.inception_v3 import inception_v3_base, inception_v3_arg_scope
    tl.layers.set_name_reuse(reuse)
    with slim.arg_scope(inception_v3_arg_scope()):
        net_img_in = tl.layers.InputLayer(input_imgs, name='input_image_layer')
        network = tl.layers.SlimNetsLayer(layer=net_img_in, slim_layer=inception_v3,
                                          slim_args= {
                                                 'trainable' : is_train,
                                                 'is_training' : is_train,
                                                 'reuse' : reuse,
                                                },
                                            name='InceptionV3')# (64, 2048)
    return network

def cnn_embed(network, reuse):
    with tf.variable_scope("rnn", reuse=reuse):
        network = DenseLayer(network, n_units = K,          # <- update with RNN
                        act = tf.identity, W_init = initializer,
                        b_init = None, name='image_embedding')
    return network

def rnn_embed(input_seqs, is_train, reuse, return_embed=True):
    """MY IMPLEMENTATION, same weights for the Word Embedding and RNN in the discriminator and generator.
    """
    w_init = tf.random_normal_initializer(stddev=0.02)
    # w_init = tf.constant_initializer(value=0.0)
    with tf.variable_scope("rnn", reuse=reuse):
        tl.layers.set_name_reuse(reuse)
        network = EmbeddingInputlayer(
                     inputs = input_seqs,
                     vocabulary_size = vocab_size,
                     embedding_size = word_embedding_size,
                     E_init = w_init,
                     name = 'wordembed')
        network = DynamicRNNLayer(network,
                     cell_fn = tf.nn.rnn_cell.LSTMCell,
                     n_hidden = rnn_hidden_size,
                     dropout = (keep_prob if is_train else None),
                     initializer = w_init,
                     sequence_length = tl.layers.retrieve_seq_length_op2(input_seqs),
                     return_last = True,
                     name = 'dynamic')
    if return_embed:
        with tf.variable_scope("rnn", reuse=reuse):
            net_embed = DenseLayer(network, n_units = K,
                            act = tf.identity, W_init = initializer,
                            b_init = None, name='hidden_state_embedding')
            return net_embed
    else:
        return network

## GAN
# small network for flower
def generator_txt2img(input_z, net_rnn_embed=None, is_train=True, reuse=False):
    # IMPLEMENTATION based on : https://github.com/paarthneekhara/text-to-image/blob/master/model.py
    s = image_size
    s2, s4, s8, s16 = int(s/2), int(s/4), int(s/8), int(s/16)
    gf_dim = 64         # Number of conv in the first layer generator 64 Torch code use 128 : line 29

    w_init = tf.random_normal_initializer(stddev=0.02)
    gamma_init = tf.random_normal_initializer(1., 0.02)

    with tf.variable_scope("generator", reuse=reuse):
        tl.layers.set_name_reuse(reuse)
        net_in = InputLayer(input_z, name='g_inputz')

        if net_rnn_embed is not None:
            # paper 4.1 : the discription embedding is first compressed using a FC layer to small dim (128), followed by leaky-Relu
            net_reduced_text = DenseLayer(net_rnn_embed, n_units=t_dim,
                    act=lambda x: tl.act.lrelu(x, 0.2),
                    W_init = w_init, name='g_reduce_text/dense')
            # paper 4.1 : and then concatenated to the noise vector z
            net_in = ConcatLayer([net_in, net_reduced_text], concat_dim=1, name='g_concat_z_seq')
        else:
            print("No text info will be used, i.e. normal DCGAN")

        net_h0 = DenseLayer(net_in, gf_dim*8*s16*s16, act=tf.identity,
                W_init=w_init, name='g_h0/dense')                  # (64, 8192)
        net_h0 = ReshapeLayer(net_h0, [-1, s16, s16, gf_dim*8], name='g_h0/reshape')
        net_h0 = BatchNormLayer(net_h0, act=tf.nn.relu, is_train=is_train,
                gamma_init=gamma_init, name='g_h0/batch_norm')

        net_h1 = DeConv2d(net_h0, gf_dim*4, (5, 5), out_size=(s8, s8), strides=(2, 2),
                padding='SAME', batch_size=batch_size, act=None, W_init=w_init, name='g_h1/decon2d')
        net_h1 = BatchNormLayer(net_h1, act=tf.nn.relu, is_train=is_train,
                gamma_init=gamma_init, name='g_h1/batch_norm')

        net_h2 = DeConv2d(net_h1, gf_dim*2, (5, 5), out_size=(s4, s4), strides=(2, 2),
                padding='SAME', batch_size=batch_size, act=None, W_init=w_init, name='g_h2/decon2d')
        net_h2 = BatchNormLayer(net_h2, act=tf.nn.relu, is_train=is_train,
                gamma_init=gamma_init, name='g_h2/batch_norm')

        net_h3 = DeConv2d(net_h2, gf_dim, (5, 5), out_size=(s2, s2), strides=(2, 2),
                padding='SAME', batch_size=batch_size, act=None, W_init=w_init, name='g_h3/decon2d')
        net_h3 = BatchNormLayer(net_h3, act=tf.nn.relu, is_train=is_train,
                gamma_init=gamma_init, name='g_h3/batch_norm')

        net_h4 = DeConv2d(net_h3, c_dim, (5, 5), out_size=(s, s), strides=(2, 2),
                padding='SAME', batch_size=batch_size, act=None, W_init=w_init, name='g_h4/decon2d')
        logits = net_h4.outputs
        # net_h4.outputs = tf.nn.sigmoid(net_h4.outputs)  # DCGAN uses tanh
        net_h4.outputs = tf.nn.tanh(net_h4.outputs)
    return net_h4, logits

def discriminator_txt2img(input_images, net_rnn_embed=None, is_train=True, reuse=False):
    # IMPLEMENTATION based on : https://github.com/paarthneekhara/text-to-image/blob/master/model.py
    #       https://github.com/reedscot/icml2016/blob/master/main_cls_int.lua
    w_init = tf.random_normal_initializer(stddev=0.02)
    gamma_init=tf.random_normal_initializer(1., 0.02)
    df_dim = 64         # Number of conv in the first layer discriminator 64

    with tf.variable_scope("discriminator", reuse=reuse):
        tl.layers.set_name_reuse(reuse)

        net_in = InputLayer(input_images, name='d_input/images')
        net_h0 = Conv2d(net_in, df_dim, (5, 5), (2, 2), act=lambda x: tl.act.lrelu(x, 0.2),
                padding='SAME', W_init=w_init, name='d_h0/conv2d')  # (64, 32, 32, 64)

        net_h1 = Conv2d(net_h0, df_dim*2, (5, 5), (2, 2), act=None,
                padding='SAME', W_init=w_init, name='d_h1/conv2d')
        net_h1 = BatchNormLayer(net_h1, act=lambda x: tl.act.lrelu(x, 0.2),
                is_train=is_train, gamma_init=gamma_init, name='d_h1/batchnorm') # (64, 16, 16, 128)

        net_h2 = Conv2d(net_h1, df_dim*4, (5, 5), (2, 2), act=None,
                padding='SAME', W_init=w_init, name='d_h2/conv2d')
        net_h2 = BatchNormLayer(net_h2, act=lambda x: tl.act.lrelu(x, 0.2),
                is_train=is_train, gamma_init=gamma_init, name='d_h2/batchnorm')    # (64, 8, 8, 256)

        net_h3 = Conv2d(net_h2, df_dim*8, (5, 5), (2, 2), act=None,
                padding='SAME', W_init=w_init, name='d_h3/conv2d')
        net_h3 = BatchNormLayer(net_h3, act=lambda x: tl.act.lrelu(x, 0.2),
                is_train=is_train, gamma_init=gamma_init, name='d_h3/batchnorm') # (64, 4, 4, 512)  paper 4.1: when the spatial dim of the D is 4x4, we replicate the description embedding spatially and perform a depth concatenation

        if net_rnn_embed is not None:
            # paper : reduce the dim of description embedding in (seperate) FC layer followed by rectification
            net_reduced_text = DenseLayer(net_rnn_embed, n_units=t_dim,
                   act=lambda x: tl.act.lrelu(x, 0.2),
                   W_init=w_init,# b_init = None,  #<-
                   name='d_reduce_txt/dense')
            # net_rnn_embed.outputs = tl.act.lrelu(net_rnn_embed.outputs, 0.2)
                # net_reduced_text = net_rnn_embed  # if reduce_txt in rnn_embed
            ## no TL
            # net_reduced_text.outputs = tf.expand_dims(net_reduced_text.outputs, 1)
            # net_reduced_text.outputs = tf.expand_dims(net_reduced_text.outputs, 2)
            # net_reduced_text.outputs = tf.tile(net_reduced_text.outputs, [1, 4, 4, 1], name='d_tiled_embeddings')
            ## TL
            net_reduced_text = ExpandDimsLayer(net_reduced_text, axis=1, name='expand_dims1')
            net_reduced_text = ExpandDimsLayer(net_reduced_text, axis=2, name='expand_dims2')
            net_reduced_text = TileLayer(net_reduced_text, multiples=[1, 4, 4, 1], name='tile')

            net_h3_concat = ConcatLayer([net_h3, net_reduced_text], concat_dim=3, name='d_h3_concat') # (64, 4, 4, 640)
            # net_h3_concat = net_h3 # no text info
            net_h3 = Conv2d(net_h3_concat, df_dim*8, (1, 1), (1, 1), padding='VALID', W_init=w_init, name='d_h3/conv2d_2')   # paper 4.1: perform 1x1 conv followed by rectification and a 4x4 conv to compute the final score from D
            net_h3 = BatchNormLayer(net_h3, act=lambda x: tl.act.lrelu(x, 0.2),
                    is_train=is_train, gamma_init=gamma_init, name='d_h3/batch_norm_2') # (64, 4, 4, 512)
        else:
            print("No text info will be used, i.e. normal DCGAN")

        net_h4 = FlattenLayer(net_h3, name='d_h4/flatten')          # (64, 8192)
        net_h4 = DenseLayer(net_h4, n_units=1, act=tf.identity,
                W_init = w_init, name='d_h4/dense')
        logits = net_h4.outputs
        net_h4.outputs = tf.nn.sigmoid(net_h4.outputs)  # (64, 1)
    return net_h4, logits

# large network for MSCOCO
def generator_txt2img_resnet(input_z, net_rnn_embed=None, is_train=True, reuse=False, is_large=False):
    # Generator with ResNet : line 93 https://github.com/reedscot/icml2016/blob/master/main_cls.lua
    s = image_size # output image size [64]
    s2, s4, s8, s16 = int(s/2), int(s/4), int(s/8), int(s/16)
    gf_dim = 400#256 #196    # <- gen filters in first conv layer [196] https://github.com/reedscot/icml2016/blob/master/scripts/train_coco.sh

    w_init = tf.random_normal_initializer(stddev=0.02)  # 73
    gamma_init = tf.random_normal_initializer(1., 0.02) # 74

    with tf.variable_scope("generator", reuse=reuse):
        tl.layers.set_name_reuse(reuse)
        net_in = InputLayer(input_z, name='g_inputz')

        if net_rnn_embed is not None:
            net_rnn_embed = DenseLayer(net_rnn_embed, n_units=t_dim,            # 95, 96
                    act=lambda x: tl.act.lrelu(x, 0.2), W_init = w_init, name='g_reduce_text/dense')
            net_in = ConcatLayer([net_in, net_rnn_embed], concat_dim=1, name='g_concat_z_seq') # 103  (64, 356) - 100+256=356
        else:
            print("No text info is used, i.e. DCGAN")

        ## 105 Note: ppwwyyxx - SpatialFullConvolution on 1x1 input is equivalent to a dense layer.
        net_h0 = DenseLayer(net_in, gf_dim*8*s16*s16, act=tf.identity,          # 106 netG:add(SpatialFullConvolution(opt.nz + opt.nt, ngf * 8, 4, 4))
                W_init=w_init, name='g_h0/dense')                               # (64, gf_dim*8*4*4)
        net_h0 = ReshapeLayer(net_h0, [-1, s16, s16, gf_dim*8], name='g_h0/reshape')# 106  (64, 4, 4, gf_dim*8) = (ngf*8) x 4 x 4
        net_h0 = BatchNormLayer(net_h0,  #act=tf.nn.relu,                       # 107 no relu
                is_train=is_train, gamma_init=gamma_init, name='g_h0/batch_norm')

        # 109 resnet
        net_h1 = Conv2d(net_h0, gf_dim*2, (1, 1), (1, 1),                       # 112 (64, 4, 4, gf_dim*2)
                padding='VALID', act=None, W_init=w_init, name='g_h1/conv2d')
        net_h1 = BatchNormLayer(net_h1, act=tf.nn.relu, is_train=is_train,      # 113
                gamma_init=gamma_init, name='g_h1/batch_norm')
        net_h2 = Conv2d(net_h1, gf_dim*2, (3, 3), (1, 1),                       # 114
                padding='SAME', act=None, W_init=w_init, name='g_h2/conv2d')
        net_h2 = BatchNormLayer(net_h2, act=tf.nn.relu, is_train=is_train,      # 115
                gamma_init=gamma_init, name='g_h2/batch_norm')
        net_h3 = Conv2d(net_h2, gf_dim*8, (3, 3), (1, 1),                       # 116
                padding='SAME', act=None, W_init=w_init, name='g_h3/conv2d')
        net_h3 = BatchNormLayer(net_h3, # act=tf.nn.relu,                       # 117 no relu
                is_train=is_train, gamma_init=gamma_init, name='g_h3/batch_norm')
        net_h3.outputs = tf.add(net_h3.outputs, net_h0.outputs)                        # 121 (64, 4, 4, gf_dim*8) = (ngf*8) x 4 x 4
        # 121 end resnet

        if is_large is True: # 123 resnet
            net_h = Conv2d(net_h3, gf_dim*2, (1, 1), (1, 1),                    # 127
                    padding='VALID', act=None, W_init=w_init, name='g_h3/conv2d2')
            net_h = BatchNormLayer(net_h, act=tf.nn.relu, is_train=is_train,    # 128
                    gamma_init=gamma_init, name='g_h3/batch_norm2')
            net_h = Conv2d(net_h, gf_dim*2, (3, 3), (1, 1),                     # 129
                    padding='SAME', act=None, W_init=w_init, name='g_h3/conv2d3')
            net_h = BatchNormLayer(net_h, act=tf.nn.relu, is_train=is_train,    # 130
                    gamma_init=gamma_init, name='g_h3/batch_norm3')
            net_h = Conv2d(net_h, gf_dim*8, (3, 3), (1, 1),                     # 131
                    padding='SAME', act=None, W_init=w_init, name='g_h3/conv2d4')
            net_h = BatchNormLayer(net_h, #act=tf.nn.relu,                      # 132
                    is_train=is_train, gamma_init=gamma_init, name='g_h3/batch_norm4')
            net_h3.outputs = tf.add(net_h3.outputs, net_h.outputs)                    # 136 (64, 4, 4, gf_dim*8) = (ngf*8) x 4 x 4
        net_h3.outputs = tf.nn.relu(net_h3.outputs)                             # 139

        net_h4 = DeConv2d(net_h3, gf_dim*4, (4, 4), out_size=(s8, s8), strides=(2, 2),# 142 (64, 8, 8, gf_dim*4) = (ngf*4) x 4 x 4
                padding='SAME', batch_size=batch_size, act=None, W_init=w_init, name='g_h4/decon2d')
        net_h4 = BatchNormLayer(net_h4,# act=tf.nn.relu,                        # 143 no relu
                is_train=is_train, gamma_init=gamma_init, name='g_h4/batch_norm')

        # 148 resnet (ngf*4) x 8 x 8 = (64, 8, 8, 512)
        net_h5 = Conv2d(net_h4, gf_dim, (1, 1), (1, 1),                         # 148
                padding='VALID', act=None, W_init=w_init, name='g_h5/conv2d')
        net_h5 = BatchNormLayer(net_h5, act=tf.nn.relu, is_train=is_train,      # 149
                gamma_init=gamma_init, name='g_h5/batch_norm')
        net_h6 = Conv2d(net_h5, gf_dim, (3, 3), (1, 1),                         # 150
                padding='SAME', act=None, W_init=w_init, name='g_h6/conv2d')
        net_h6 = BatchNormLayer(net_h6, act=tf.nn.relu, is_train=is_train,      # 151
                gamma_init=gamma_init, name='g_h6/batch_norm')
        net_h7 = Conv2d(net_h6, gf_dim*4, (3, 3), (1, 1),                       # 152
                padding='SAME', act=None, W_init=w_init, name='g_h7/conv2d')
        net_h7 = BatchNormLayer(net_h7, #act=tf.nn.relu,                        # 153 no relu
                is_train=is_train, gamma_init=gamma_init, name='g_h7/batch_norm')
        net_h7.outputs = tf.add(net_h4.outputs, net_h7.outputs)                        # 157 (64, 8, 8, gf_dim*4) = (ngf*4) x 8 x 8
        # 158 end resnet

        if is_large is True:# 159 resnet
            net_h = Conv2d(net_h7, gf_dim, (1, 1), (1, 1),                      # 163
                   padding='VALID', act=None, W_init=w_init, name='g_h7/conv2d1')
            net_h = BatchNormLayer(net_h, act=tf.nn.relu, is_train=is_train,    # 164
                   gamma_init=gamma_init, name='g_h7/batch_norm1')
            net_h = Conv2d(net_h, gf_dim, (3, 3), (1, 1),                       # 165
                   padding='SAME', act=None, W_init=w_init, name='g_h7/conv2d2')
            net_h = BatchNormLayer(net_h, act=tf.nn.relu, is_train=is_train,    # 166
                   gamma_init=gamma_init, name='g_h7/batch_norm2')
            net_h = Conv2d(net_h, gf_dim*4, (3, 3), (1, 1),                     # 167
                   padding='SAME', act=None, W_init=w_init, name='g_h7/conv2d3')
            net_h = BatchNormLayer(net_h, #act=tf.nn.relu,                      # 168 no relu
                   is_train=is_train, gamma_init=gamma_init, name='g_h7/batch_norm3')
            net_h7.outputs = tf.add(net_h.outputs, net_h7.outputs)                     # 172
        net_h7.outputs = tf.nn.relu(net_h7.outputs)                             # 175 (64, 8, 8, gf_dim*4) = (ngf*4) x 8 x 8

        net_h8 = DeConv2d(net_h7, gf_dim*2, (4, 4), out_size=(s4, s4), strides=(2, 2),# 178 (64, 16, 16, gf_dim*2) = (ngf*2) x 16 x 16
                padding='SAME', batch_size=batch_size, act=None, W_init=w_init, name='g_h8/decon2d')
        net_h8 = BatchNormLayer(net_h8, act=tf.nn.relu, is_train=is_train,      # 179, 180
                gamma_init=gamma_init, name='g_h8/batch_norm')

        ## zsdonghao add    # same images were found
        # net_h = Conv2d(net_h8, gf_dim/2, (1, 1), (1, 1),
        #         padding='VALID', act=None, W_init=w_init, name='g_h8/conv2d2')
        # net_h = BatchNormLayer(net_h, act=tf.nn.relu, is_train=is_train,
        #         gamma_init=gamma_init, name='g_h8/batch_norm2')
        # net_h = Conv2d(net_h, gf_dim/2, (3, 3), (1, 1),
        #         padding='SAME', act=None, W_init=w_init, name='g_h8/conv2d3')
        # net_h = BatchNormLayer(net_h, act=tf.nn.relu, is_train=is_train,
        #         gamma_init=gamma_init, name='g_h8/batch_norm3')
        # net_h = Conv2d(net_h, gf_dim*2, (3, 3), (1, 1),
        #         padding='SAME', act=None, W_init=w_init, name='g_h8/conv2d4')
        # net_h = BatchNormLayer(net_h, #act=tf.nn.relu,
        #         is_train=is_train, gamma_init=gamma_init, name='g_h8/batch_norm4')
        # net_h8.outputs = net_h.outputs + net_h8.outputs
        # net_h8.outputs = tf.nn.relu(net_h8.outputs)
        ## end zsdonghao

        net_h9 = DeConv2d(net_h8, gf_dim, (4, 4), out_size=(s2, s2), strides=(2, 2),# 183 (64, 32, 32, gf_dim) = (ngf) x 32 x 32
                padding='SAME', batch_size=batch_size, act=None, W_init=w_init, name='g_h9/decon2d')
        net_h9 = BatchNormLayer(net_h9, act=tf.nn.relu, is_train=is_train,      # 184
                gamma_init=gamma_init, name='g_h9/batch_norm')

        ## zsdonghao add    # same images were found
        # net_h = Conv2d(net_h9, gf_dim/4, (1, 1), (1, 1),
        #         padding='VALID', act=None, W_init=w_init, name='g_h9/conv2d2')
        # net_h = BatchNormLayer(net_h, act=tf.nn.relu, is_train=is_train,
        #         gamma_init=gamma_init, name='g_h9/batch_norm2')
        # net_h = Conv2d(net_h, gf_dim/4, (3, 3), (1, 1),
        #         padding='SAME', act=None, W_init=w_init, name='g_h9/conv2d3')
        # net_h = BatchNormLayer(net_h, act=tf.nn.relu, is_train=is_train,
        #         gamma_init=gamma_init, name='g_h9/batch_norm3')
        # net_h = Conv2d(net_h, gf_dim, (3, 3), (1, 1),
        #         padding='SAME', act=None, W_init=w_init, name='g_h9/conv2d4')
        # net_h = BatchNormLayer(net_h, #act=tf.nn.relu,
        #         is_train=is_train, gamma_init=gamma_init, name='g_h9/batch_norm4')
        # net_h9.outputs = net_h.outputs + net_h9.outputs
        # net_h9.outputs = tf.nn.relu(net_h9.outputs)
        ## end zsdonghao

        net_ho = DeConv2d(net_h9, c_dim, (4, 4), out_size=(s, s), strides=(2, 2),# 187 (64, 64, 64, 3)
                padding='SAME', batch_size=batch_size, act=None, W_init=w_init, name='g_ho/decon2d')
        logits = net_ho.outputs
        net_ho.outputs = tf.nn.tanh(net_ho.outputs)                             # 188
    return net_ho, logits

def discriminator_txt2img_resnet(input_images, net_rnn_embed=None, is_train=True, reuse=False):
    # Discriminator with ResNet : line 197 https://github.com/reedscot/icml2016/blob/master/main_cls.lua
    w_init = tf.random_normal_initializer(stddev=0.02)  # 73
    gamma_init=tf.random_normal_initializer(1., 0.02)   # 74
    df_dim = 196         # number of conv in the first layer discriminator [196] https://github.com/reedscot/icml2016/blob/master/scripts/train_coco.sh

    with tf.variable_scope("discriminator", reuse=reuse):
        tl.layers.set_name_reuse(reuse)
        # (nc) x 64 x 64
        net_in = InputLayer(input_images, name='d_input/images')
        net_h0 = Conv2d(net_in, df_dim, (4, 4), (2, 2), act=lambda x: tl.act.lrelu(x, 0.2), # 199
                padding='SAME', W_init=w_init, name='d_h0/conv2d')              # (64, 32, 32, 64) = (ndf) x 32 x 32

        net_h1 = Conv2d(net_h0, df_dim*2, (4, 4), (2, 2), act=None,             # 203  (64, 16, 16, df_dim*2) = (ndf*2) x 16 x 16
                padding='SAME', W_init=w_init, name='d_h1/conv2d')
        net_h1 = BatchNormLayer(net_h1, act=lambda x: tl.act.lrelu(x, 0.2),     # 204, 205
                is_train=is_train, gamma_init=gamma_init, name='d_h1/batchnorm')
        net_h2 = Conv2d(net_h1, df_dim*4, (4, 4), (2, 2), act=None,             # 208  (64, 8, 8, df_dim*4) = (ndf*4) x 8 x 8
                padding='SAME', W_init=w_init, name='d_h2/conv2d')
        net_h2 = BatchNormLayer(net_h2, act=lambda x: tl.act.lrelu(x, 0.2),     # 209, 210
                is_train=is_train, gamma_init=gamma_init, name='d_h2/batchnorm')
        net_h3 = Conv2d(net_h2, df_dim*8, (4, 4), (2, 2), act=None,             # 213  (64, 4, 4, df_dim*8) = (ndf*8) x 4 x 4
                padding='SAME', W_init=w_init, name='d_h3/conv2d')
        net_h3 = BatchNormLayer(net_h3, #act=lambda x: tl.act.lrelu(x, 0.2),    # 214, no lrelu
                is_train=is_train, gamma_init=gamma_init, name='d_h3/batchnorm')

        # 216 resnet
        net_h = Conv2d(net_h3, df_dim*2, (1, 1), (1, 1), act=None,              # 219
                padding='VALID', W_init=w_init, name='d_h3/conv2d2')
        net_h = BatchNormLayer(net_h, act=lambda x: tl.act.lrelu(x, 0.2),       # 220
                is_train=is_train, gamma_init=gamma_init, name='d_h3/batchnorm2')
        net_h = Conv2d(net_h, df_dim*2, (3, 3), (1, 1), act=None,               # 221
                padding='SAME', W_init=w_init, name='d_h3/conv2d3')
        net_h = BatchNormLayer(net_h, act=lambda x: tl.act.lrelu(x, 0.2),       # 222
                is_train=is_train, gamma_init=gamma_init, name='d_h3/batchnorm3')
        net_h = Conv2d(net_h, df_dim*8, (3, 3), (1, 1), act=None,               # 223
                padding='SAME', W_init=w_init, name='d_h3/conv2d4')
        net_h = BatchNormLayer(net_h, #act=lambda x: tl.act.lrelu(x, 0.2),      # 224
                is_train=is_train, gamma_init=gamma_init, name='d_h3/batchnorm4')
        net_h3.outputs = tl.act.lrelu( tf.add(net_h.outputs, net_h3.outputs), 0.2)      # 228, 230 (64, 4, 4, df_dim*8)
        # 228 end resnet

        if net_rnn_embed is not None:  # 232
            net_reduced_text = DenseLayer(net_rnn_embed, n_units=t_dim,         # 233, 234
                   act=lambda x: tl.act.lrelu(x, 0.2),
                   W_init=w_init, name='d_reduce_txt/dense')
            ## no TL
            # net_reduced_text.outputs = tf.expand_dims(net_reduced_text.outputs, 1) # 235
            # net_reduced_text.outputs = tf.expand_dims(net_reduced_text.outputs, 2) # 236
            # net_reduced_text.outputs = tf.tile(net_reduced_text.outputs, [1, 4, 4, 1], name='d_tiled_embeddings')
            ## TL
            net_reduced_text = ExpandDimsLayer(net_reduced_text, axis=1, name='expand_dims1')
            net_reduced_text = ExpandDimsLayer(net_reduced_text, axis=2, name='expand_dims2')
            net_reduced_text = TileLayer(net_reduced_text, multiples=[1, 4, 4, 1], name='tile')

            net_h3_concat = ConcatLayer([net_h3, net_reduced_text], concat_dim=3, name='d_h3_concat') # 242  if t_dim = 256 : (64, 4, 4, 786); if t_dim = 128 :(64, 4, 4, 640)
            # 243 (ndf*8 + 128 or 256) x 4 x 4
            net_h3 = Conv2d(net_h3_concat, df_dim*8, (1, 1), (1, 1),            # 244 (64, 4, 4, df_dim*8)
                    padding='VALID', W_init=w_init, name='d_h3/conv2d_2')
            net_h3 = BatchNormLayer(net_h3, act=lambda x: tl.act.lrelu(x, 0.2), # 245
                    is_train=is_train, gamma_init=gamma_init, name='d_h3/batch_norm_2')
        else:
            print("No text info is used, i.e. DCGAN")
        net_h4 = Conv2d(net_h3, 1, (4, 4), (1, 1), padding='VALID', W_init=w_init, name='d_h4/conv2d_2') # 246 (64, 1, 1, 1), if padding='SAME' (64, 4, 4, 1)
        # 1 x 1 x 1
        net_h4 = FlattenLayer(net_h4, name='d_h4/flatten')                      # 249 (64, 1)
        logits = net_h4.outputs
        net_h4.outputs = tf.nn.sigmoid(net_h4.outputs)
    return net_h4, logits


def main(_):
    # Model checkpoint file or directory containing a model checkpoint file.
    checkpoint_path = CHECKPOINT_DIR
    # Text file containing the vocabulary.
    vocab_file = VOCAB_FILE
    # File pattern or comma-separated list of file patterns of image files.
    mode = 'inference'  # mode of im2txt module, generating image captions
    top_k = 2
    print("n_images_train: %d" % n_images_train)

    # g = tf.Graph()
    # with g.as_default():

    ## Build graph for im2txt
    # images, input_seqs, target_seqs, input_mask, input_feed = Build_Inputs(mode, input_file_pattern=None)
    images = tf.placeholder('float32', [batch_size, image_height, image_width, 3])
    input_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name='input_seqs')
    net_image_embeddings = Build_Image_Embeddings(mode, images, train_inception=False)
    net_seq_embeddings = Build_Seq_Embeddings(input_seqs)
    softmax, net_img_rnn, net_seq_rnn, state_feed = Build_Model(mode, net_image_embeddings, net_seq_embeddings, target_seqs=None, input_mask=None)

    if tf.gfile.IsDirectory(checkpoint_path):
        # checkpoint_path = tf.train.latest_checkpoint(checkpoint_path)
        checkpoint_path = checkpoint_path + '/model.ckpt-1000000'
        if not checkpoint_path:
            raise ValueError("No im2txt checkpoint file found in: %s" % checkpoint_path)

    saver = tf.train.Saver()
    def _restore_fn(sess):
        tf.logging.info("Loading model from im2txt checkpoint: %s", checkpoint_path)
        saver.restore(sess, checkpoint_path)
        tf.logging.info("Successfully loaded im2txt checkpoint: %s",
                      os.path.basename(checkpoint_path))

    restore_fn = _restore_fn

    ## Create the vocabulary.
    vocab = tl.nlp.Vocabulary(vocab_file)

    ## Graph for image and text mapping
    t_rnn_image = tf.placeholder('float32', [batch_size, image_height, image_width, 3], name = 'image_txt/image')          # 299 [-1, 1]
    t_rnn_image_w = tf.placeholder('float32', [batch_size, image_height, image_width, 3], name = 'image_txt/image_w')
    t_rnn_caption = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name='image_txt/text')
    t_rnn_caption_w = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name='image_txt/text_w')

    net_cnn = cnn_embed(cnn(t_rnn_image, is_train=False, reuse=True), reuse=False)
    x = net_cnn.outputs
    net_rnn = rnn_embed(t_rnn_caption, is_train=True, reuse=False)
    v = net_rnn.outputs
    x_w = cnn_embed(cnn(t_rnn_image_w, is_train=False, reuse=True), reuse=True).outputs
    v_w = rnn_embed(t_rnn_caption_w, is_train=True, reuse=True).outputs

    alpha = 0.2 # margin alpha
    e_loss = tf.reduce_mean(tf.maximum(0., alpha - cosine_similarity(x, v) + cosine_similarity(x, v_w))) + \
                tf.reduce_mean(tf.maximum(0., alpha - cosine_similarity(x, v) + cosine_similarity(x_w, v)))

    ## Build graph for txt2im.
    t_real_image = tf.placeholder('float32', [batch_size, image_size, image_size, 3], name = 'real_image')
    t_wrong_image = tf.placeholder('float32', [batch_size, image_size, image_size, 3], name = 'wrong_image')
    t_real_caption = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name='real_caption_input')
    # t_wrong_caption = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name='real_wrong_input')
    t_z = tf.placeholder(tf.float32, [batch_size, z_dim], name='z_noise')

    net_rnn2 = rnn_embed(t_real_caption, is_train=False, reuse=True, return_embed=False)
    net_fake_image, _ = generator_txt2img_resnet(t_z,
                    net_rnn2,
                    is_train=True, reuse=False)
    net_d, disc_fake_image_logits = discriminator_txt2img_resnet(  # <- fake image, right text
                    net_fake_image.outputs,
                    net_rnn2,
                    is_train=True, reuse=False)
    _, disc_real_image_logits = discriminator_txt2img_resnet(  # <- real image, right text
                    t_real_image,
                    net_rnn2,
                    is_train=True, reuse=True)
    # _, disc_wrong_caption_logits = discriminator_txt2img_resnet(   # <- real image, wrong text
    #                 t_real_image,
    #                 rnn_embed(t_wrong_caption, is_train=False, reuse=True, return_embed=False),
    #                 is_train=True, reuse=True)
    _, disc_wrong_image_logits = discriminator_txt2img_resnet(   # <- wrong image, right text
                    t_wrong_image,
                    net_rnn2,
                    is_train=True, reuse=True)

    # testing inference for txt2img
    net_g, _ = generator_txt2img_resnet(t_z,
                    rnn_embed(t_real_caption, is_train=False, reuse=True, return_embed=False),
                    is_train=False, reuse=True)

    ## loss for txt2im. # real == 1, fake == 0
    d_loss1 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(disc_real_image_logits, tf.ones_like(disc_real_image_logits)))         # <- real image, right text
    # d_loss2 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(disc_wrong_caption_logits, tf.zeros_like(disc_wrong_caption_logits)))  # <- real image, wrong text
    d_loss2 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(disc_wrong_image_logits, tf.zeros_like(disc_wrong_image_logits)))      # <- wrong image, right text
    d_loss3 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(disc_fake_image_logits, tf.zeros_like(disc_fake_image_logits)))        # <- fake image, right(arbitrary) text

    cls_weight = 0.5
    d_loss = d_loss1 + cls_weight * d_loss2 + (1-cls_weight) * d_loss3

    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(disc_fake_image_logits, tf.ones_like(disc_fake_image_logits))) # real == 1, fake == 0

    net_fake_image.print_params(False)
    net_fake_image.print_layers()

    ## cost for txt2im (real = 1, fake = 0)
    lr = 0.0002     # initial learning rate for adam
    beta1 = 0.5     # momentum term of adam
    lr_decay = 0.5  # decay factor for adam, https://github.com/reedscot/icml2016/blob/master/main_cls.lua
    decay_every = 40  # https://github.com/reedscot/icml2016/blob/master/main_cls.lua
    n_step_epoch = int(80000/batch_size)
    e_vars = tl.layers.get_variables_with_name('rnn', True, True)           #  remove if DCGAN only
    d_vars = tl.layers.get_variables_with_name('discriminator', True, True)
    g_vars = tl.layers.get_variables_with_name('generator', True, True)
    # exit()

    with tf.variable_scope('learning_rate'):
        lr_v = tf.Variable(lr, trainable=False)
    d_optim = tf.train.AdamOptimizer(lr_v, beta1=beta1).minimize(d_loss, var_list=d_vars )
    g_optim = tf.train.AdamOptimizer(lr_v, beta1=beta1).minimize(g_loss, var_list=g_vars )
    # e_optim = tf.train.AdamOptimizer(lr_v, beta1=beta1).minimize(e_loss, var_list=e_vars )
    grads, _ = tf.clip_by_global_norm(tf.gradients(e_loss, e_vars), 10)
    optimizer = tf.train.AdamOptimizer(lr_v, beta1=beta1)# optimizer = tf.train.GradientDescentOptimizer(lre)
    e_optim = optimizer.apply_gradients(zip(grads, e_vars))

    # opt1 = tf.train.AdamOptimizer(lr, beta1=beta1)
    # opt2 = tf.train.AdamOptimizer(lr/10, beta1=beta1)
    # grads = tf.gradients(d_loss, d_vars + e_vars)
    # # grads, _ = tf.clip_by_global_norm(grads, 10)      # Truncated grads global
    # grads1 = grads[0:len(d_vars)]
    # grads2 = grads[-len(e_vars):]
    # # grads2, _ = = tf.clip_by_global_norm(grads2, 10)      # Truncated
    # train_op1 = opt1.apply_gradients(zip(grads1, d_vars))
    # train_op2 = opt2.apply_gradients(zip(grads2, e_vars))
    # d_optim = tf.group(train_op1, train_op2)

    # g.finalize()

    ## Seed
    sample_size = batch_size
    sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, z_dim)).astype(np.float32)
        # sample_seed = np.random.uniform(low=-1, high=1, size=(sample_size, z_dim)).astype(np.float32)
    # sample_sentence = ["a person walking across a beach next to the ocean.",
    #                     "a man swinging a baseball bat over home plate."] * int(sample_size/2)
    sample_sentence = ["a person walking across a beach next to the ocean."] * int(sample_size/8) + \
                      ["a man swinging a baseball bat over home plate."] * int(sample_size/8) + \
                      ["a close up of a person eating a hot dog."] * int(sample_size/8) + \
                      ["two giraffes that are standing together in a field."] * int(sample_size/8) + \
                      ["a large passenger plane on a runway prepares to taxi"] * int(sample_size/8) + \
                      ["a toilet in a small room with a window and unfinished walls."] * int(sample_size/8) + \
                      ["a group of people on skis stand on the snow."] * int(sample_size/8) + \
                      ["a man in a wet suit riding a surfboard on a wave."] * int(sample_size/8)

    # sample_sentence = ["a green school bus parked in a parking lot."] * int(sample_size/8) + \
    #                   ["a elephant in the sky."] * int(sample_size/8) + \
    #                   ["a people."] * int(sample_size/8) + \
    #                   ["two cars."] * int(sample_size/8) + \
    #                   ["a black toilet in a small black room with a window and unfinished black walls."] * int(sample_size/8) + \
    #                   ["a red toilet in a small red room with a window and unfinished red walls."] * int(sample_size/8) + \
    #                   ["a green toilet in a small green room with a window and unfinished green walls."] * int(sample_size/8) + \
    #                   ["a yellow toilet in a small yellow room with a window and unfinished yellow walls."] * int(sample_size/8)
    # sample_sentence = captions_ids_test[0:sample_size]
    for i, sentence in enumerate(sample_sentence):
        sample_sentence[i] = [vocab.word_to_id(word) for word in nltk.tokenize.word_tokenize(sentence)] + [vocab.end_id] # <- end_id
        print("seed: %s" % sentence)
    sample_sentence = tl.prepro.pad_sequences(sample_sentence, padding='post')

    # Train txt2im
    # with tf.Session(graph=g) as sess:
    with tf.Session() as sess:
        sess.run(tf.initialize_all_variables())
        ## Restore the im2txt model from checkpoint.
        restore_fn(sess)

        ## Restore the txt2im model from checkpoint
        net_c_name = os.path.join(save_dir, 'net_c.npz')
        net_e_name = os.path.join(save_dir, 'net_e.npz')
        net_g_name = os.path.join(save_dir, 'net_g.npz')
        net_d_name = os.path.join(save_dir, 'net_d.npz')
        if True:
            if not (os.path.exists(net_e_name) and os.path.exists(net_c_name)):
                print("[!] Loading RNN checkpoints failed!")
            else:
                net_c_loaded_params = tl.files.load_npz(name=net_c_name)
                tl.files.assign_params(sess, net_c_loaded_params, net_cnn)
                net_e_loaded_params = tl.files.load_npz(name=net_e_name)
                tl.files.assign_params(sess, net_e_loaded_params, net_rnn)
                print("[*] Loading RNN checkpoints SUCCESS!")
            if not (os.path.exists(net_g_name) and os.path.exists(net_d_name)):
                print("[!] Loading G and D checkpoints failed!")
            else:
                net_g_loaded_params = tl.files.load_npz(name=net_g_name)
                net_d_loaded_params = tl.files.load_npz(name=net_d_name)
                tl.files.assign_params(sess, net_g_loaded_params, net_g)
                tl.files.assign_params(sess, net_d_loaded_params, net_d)
                print("[*] Loading G and D checkpoints SUCCESS!")

        max_caption_length = 25
        n_step = 1000000
        n_check_step = 500
        total_d_loss, total_g_loss, total_e_loss = 0, 0, 0
        total_e_loss = 0
        total_d1, total_d2, total_d3 = 0, 0, 0
        errD, d1, d2, d3, errG, errE = 0, 0, 0, 0, 0, 0
        for step in range(0, n_step):
            ## update RNN learning rate
            if step !=0 and ((step / n_step_epoch) % decay_every == 0):
                new_lr_decay = lr_decay ** ((step / n_step_epoch) // decay_every)
                sess.run(tf.assign(lr_v, lr * new_lr_decay))
                log = " ** new learning rate: %f" % (lr * new_lr_decay)
                print(log)
                logging.debug(log)
            elif step == 0:
                log = " ** init lr: %f, n_step_epoch: %d, decay_every_epoch: %d, lr_decay: %f" % (lr, n_step_epoch, decay_every, lr_decay)
                print(log)
                logging.debug(log)

            start_time = time.time()

            ### im2txt
            idexs = get_random_int(min=0, max=n_images_train-1, number=batch_size)
            b_image_file_name = images_train_list[idexs]

            # temp_time = time.time()
            ## read a batch of images for folder
            b_images = threading_data(b_image_file_name, prepro_img, mode='read_image')
                # print('im2txt read image %f'% (time.time() - temp_time))  # <-- 0.08 for 64 imgs
                # temp_time = time.time()
                ## you may want to view the original image
                # for i, img in enumerate(b_images):
                #     scipy.misc.imsave('image_orig_%d.png' % i, img)
                ## preprocess a batch of image
            b_images = threading_data(b_images, prepro_img, mode='random_size_to_346')
                # print('im2txt random2fixsize image %f' % (time.time() - temp_time))   # <-- 0.06 for 64 imgs
                # temp_time = time.time()
            b_images_im2txt = threading_data(b_images, prepro_img, mode='346_central_crop_to_299') # image2text
                # print(b_images_im2txt.shape, np.min(b_images_im2txt), np.max(b_images_im2txt))
                # print('im2txt 346_random_crop_to_299 image %f' % (time.time() - temp_time))   # <-- 0.05 for 64 imgs
                # temp_time = time.time()
                ## you may want to view the original image after central crop and rescale
                # for i, img in enumerate(b_images_im2txt):
                #     scipy.misc.imsave('image_im2txt%d.png' % i, img)
                # generate captions for a batch of images
                # temp_time = time.time()

            idexs_w = get_random_int(min=0, max=n_images_train-1, number=batch_size)
            b_image_file_name_w = images_train_list[idexs_w]
            b_images_w = threading_data(b_image_file_name_w, prepro_img, mode='read_image')
            b_images_w = threading_data(b_images_w, prepro_img, mode='random_size_to_346')
            b_images_rnn_w = threading_data(b_images_w, prepro_img, mode='346_random_flip_crop_to_299')
            b_images_txt2im_w = threading_data(b_images_rnn_w, prepro_img, mode='resize_to_64')

            init_state = sess.run(net_img_rnn.final_state, feed_dict={images: b_images_im2txt})
            state = np.hstack((init_state.c, init_state.h)) # (1, 1024)
            ids = [[vocab.start_id]] * batch_size

            b_sentences = [[] for _ in range(batch_size)]
            b_sentences_ids = [[] for _ in range(batch_size)]
            for _ in range(max_caption_length - 1):
                softmax_output, state = sess.run([softmax, net_seq_rnn.final_state],
                                        feed_dict={ input_seqs : ids,
                                                    state_feed : state,
                                                    })
                state = np.hstack((state.c, state.h))
                ids = []
                temp = threading_data(softmax_output, sample_fn, top_k=top_k, vocab=vocab)    # <-- not a lot faster
                i = 0
                for a_id, word in temp:
                # for i in range(batch_size):
                    # a_id = tl.nlp.sample_top(softmax_output[i], top_k=top_k)
                    # word = vocab.id_to_word(a_id)
                    b_sentences[i].append(word)
                    b_sentences_ids[i].append(int(a_id))
                    ids = ids + [[a_id]]
                    i = i + 1
            # print("im2txt sample from top k (1 step): %f" % (time.time()-temp_time)) # <-- 0.008 ~ 0.024s (x max_caption_length) for 62 img
            # temp_time = time.time()
            # print('im2txt get caption %f'% (time.time() - temp_time))   # <-- 0.xx 32 imgs
            # temp_time = time.time()
            # before cleaning caption data
            # for i, sentence in enumerate(b_sentences):
            #     print("%d : %s" % (i, " ".join(sentence)))

            ## cleaning caption data
            b_sentences_ids = process_sequences(b_sentences_ids, end_id=vocab.end_id, pad_val=0, is_shorten=True, remain_end_id=True)

            ## txt2im : Train GAN
            # read image    : b_images_txt2im
            # read caption  : b_sentences_ids
            # wrong caption : b_wrong_sentences_ids
            # print(b_images_im2txt.shape, np.min(b_images_im2txt), np.max(b_images_im2txt), b_images_im2txt.dtype)   # (32, 299, 299, 3) -1.0 1.0
            b_images_rnn = threading_data(b_images, prepro_img, mode='346_random_flip_crop_to_299')
            b_images_txt2im = threading_data(b_images_rnn, prepro_img, mode='resize_to_64')
            # print('txt2im distort images %f'% (time.time() - temp_time))    # <-- 0.11 for 64 imgs
            # temp_time = time.time()

            ## you may want to view the image after data augmentation
            # for i, img in enumerate(b_images_txt2im):
            #     scipy.misc.imsave('image_txt2im_%d.png' % i, img)
            # b_sentences_ids = process_sequences(b_sentences_ids, end_id=vocab.end_id, pad_val=0, is_shorten=True)
            # b_sentences_ids[0] = b_sentences_ids[0][2:]
            b_sentences_ids_w = b_sentences_ids[-1:]+b_sentences_ids[:-1]   # <-- the wrong captions are the real captions shift by 1
            # for i, sentence_id in enumerate(b_sentences_ids_w):
            #     print("w %d : %s" % (i, [vocab.id_to_word(id) for id in sentence_id]) )
            # exit()

            b_z = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, z_dim)).astype(np.float32)
                # b_z = np.random.uniform(low=-1, high=1, size=[batch_size, z_dim]).astype(np.float32)

            # update RNN - text-image mapping
            for _ in range(1):
                errE, _ = sess.run([e_loss, e_optim], feed_dict={
                                                t_rnn_image : b_images_rnn,
                                                t_rnn_image_w : b_images_rnn_w,
                                                t_rnn_caption : b_sentences_ids,
                                                t_rnn_caption_w : b_sentences_ids_w,
                                                })
                total_e_loss += errE

            # update D
            # errD, _ = sess.run([d_loss, d_optim], feed_dict={
            errD, d1, d2, d3, _ = sess.run([d_loss, d_loss1, d_loss2, d_loss3, d_optim], feed_dict={
                            t_real_image : b_images_txt2im,
                            # t_wrong_caption : b_sentences_ids_w,
                            t_wrong_image : b_images_txt2im_w,
                            t_real_caption : b_sentences_ids,
                            t_z : b_z})

            # update G
            errG, _ = sess.run([g_loss, g_optim], feed_dict={
                            t_real_caption : b_sentences_ids,
                            t_z : b_z})

            total_d_loss += errD; total_g_loss += errG; total_e_loss += errE
            total_d1 += d1; total_d2 += d2; total_d3 += d3
            # print('txt2im train GAN %f'% (time.time() - temp_time)) # <-- 0.19 for 64 imgs
            # temp_time = time.time()
            # print("step %d: d_loss: %.8f, g_loss: %.8f (%4.4f sec)" % (step, errD, errG, time.time()-start_time))
            # print("step %d: d_loss: %.8f, g_loss: %.8f, e_loss: %.8f (%4.4f sec)" % (step, errD, errG, errE, time.time()-start_time))
            print("step %d: d_loss: %.4f (%.3f, %.3f, %.3f), g_loss: %.4f, e_loss: %.5f (%.2f sec)" % (step, errD, d1, d2, d3, errG, errE, time.time()-start_time))
            if step != 0 and step % n_check_step == 0:
            # if step % n_check_step == 0:
                ## Print average loss
                # print(" ** d_loss: %.8f, g_loss: %.8f: " % (total_d_loss/n_check_step, total_g_loss/n_check_step))
                # log = " ** step: %d d_loss: %.8f, g_loss: %.8f: " % (step, total_d_loss/n_check_step, total_g_loss/n_check_step)
                log = " ** avg step: %d d_loss: %.4f (%.3f, %.3f, %.3f), g_loss: %.4f, e_loss: %.5f: " % (step, total_d_loss/n_check_step,
                            total_d1/n_check_step, total_d2/n_check_step, total_d3/n_check_step, total_g_loss/n_check_step, total_e_loss/n_check_step)
                            # total_d1/n_check_step, total_d2/2/n_check_step, total_d3/2/n_check_step, total_g_loss/n_check_step, total_e_loss/n_check_step)
                print(log)
                logging.debug(log)
                total_d_loss, total_g_loss, total_e_loss = 0, 0, 0
                total_d1, total_d2, total_d3 = 0, 0, 0
                ## Generate a batch of image by given seeds
                img_gen, rnn_out = sess.run([net_g.outputs, net_rnn2.outputs],
                                            feed_dict={
                                            t_real_caption : sample_sentence,
                                            t_z : sample_seed})
                # print(img_gen.shape)        # <-- (batch_size, 64, 64, 3) [-1, 1]
                print("rnn", np.min(rnn_out), np.max(rnn_out))
                save_images(img_gen, [8, int(batch_size/8)], '{}/train_{:02d}.png'.format(sample_dir, step))
                ## Save model to npz
                tl.files.save_npz(net_cnn.all_params, name=net_c_name, sess=sess)
                tl.files.save_npz(net_rnn.all_params, name=net_e_name, sess=sess)
                tl.files.save_npz(net_g.all_params, name=net_g_name, sess=sess)
                tl.files.save_npz(net_d.all_params, name=net_d_name, sess=sess)
                net_c_name_ = os.path.join(save_dir, 'net_c_%d.npz' % step)
                net_e_name_ = os.path.join(save_dir, 'net_e_%d.npz' % step)
                net_g_name_ = os.path.join(save_dir, 'net_g_%d.npz' % step)
                net_d_name_ = os.path.join(save_dir, 'net_d_%d.npz' % step)
                tl.files.save_npz(net_cnn.all_params, name=net_c_name_, sess=sess)
                tl.files.save_npz(net_rnn.all_params, name=net_e_name_, sess=sess)
                tl.files.save_npz(net_g.all_params, name=net_g_name_, sess=sess)
                tl.files.save_npz(net_d.all_params, name=net_d_name_, sess=sess)
                print("[*] Saving txt2im checkpoints SUCCESS!")


if __name__ == "__main__":
    tf.app.run()
